{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1_TASK1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = u'John met Susan in the mall. She told him that she is traveling to Europe next week.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = nlp.annotate(text, properties={\n",
    "        'annotators': 'pos,depparse,parse,lemma',\n",
    "        'outputFormat': 'json'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (NNP John))\n",
      "    (VP (VBD met)\n",
      "      (NP\n",
      "        (NP (NNP Susan))\n",
      "        (PP (IN in)\n",
      "          (NP (DT the) (NN mall)))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "print (disp['sentences'][0]['parse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a task that might benefit from NLP features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical task that will benefit from the from NLP features is sentiment analysis. The common sentiment analysis is based on every words in the corpus. This may generate a inefficiency effect and cause longer running time. For instance, with the text 'John met Susan in the mall. She told him that she is traveling to Europe next week.', the common sentiment analysis need to check every word from \"John\" to \"week\". However, only those word with sentiment should be considered in the analysis on the whole setence. With the help of NLP features, we can only focus on those that are adj, adv or negation etc. This will effectively increase the effect of ruuning sentiment analysis since we have less word to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## description on vector features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stratege of vevtorization I can think of is to use frequency of its specific level to represent the language. Similiar to the bag of words, vectorize on NLP features (POS NER Lemmas etc) requires to record the frequence of these features in corpus. With these records, the original corpus will be represented with vectors which represent the NLP features, like POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pos vevtorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "def gen():\n",
    "    yield nlp(text)\n",
    "\n",
    "matitrial = gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PROPN': 1, 'VERB': 2, 'ADP': 3, 'DET': 4, 'NOUN': 5, 'PUNCT': 6, 'PRON': 7, 'ADJ': 8}\n"
     ]
    }
   ],
   "source": [
    "#build the vocabulary\n",
    "tag_voc = dict()\n",
    "for tokens in matitrial:\n",
    "    for token in tokens:\n",
    "        tag_voc[token.pos_] = 1\n",
    "\n",
    "ind = 0\n",
    "for key, value in tag_voc.items():\n",
    "    ind += 1\n",
    "    tag_voc[key] = ind\n",
    "    \n",
    "print(tag_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dependence parsing vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarly\n",
    "def doc_generator():\n",
    "        yield nlp(text)\n",
    "\n",
    "matitrial = gen()\n",
    "\n",
    "dep_voc = dict()\n",
    "for tokens in matitrial:\n",
    "    for token in tokens:\n",
    "        dep_voc[token.dep_] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nsubj': 1, 'ROOT': 2, 'dobj': 3, 'prep': 4, 'det': 5, 'pobj': 6, 'punct': 7, 'mark': 8, 'aux': 9, 'ccomp': 10, 'amod': 11, 'npadvmod': 12}\n"
     ]
    }
   ],
   "source": [
    "ind_l = 0\n",
    "for k, v in dep_voc.items():\n",
    "    ind_l += 1\n",
    "    dep_voc[k] = ind_l\n",
    "\n",
    "print(dep_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmas vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "    yield nlp(text)\n",
    "\n",
    "matitrial = gen()\n",
    "\n",
    "lem_voc = dict()\n",
    "for tokens in matitrial:\n",
    "    for token in tokens:\n",
    "        lem_voc[token.lemma_] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'john': 1, 'meet': 2, 'susan': 3, 'in': 4, 'the': 5, 'mall': 6, '.': 7, '-PRON-': 8, 'tell': 9, 'that': 10, 'be': 11, 'travel': 12, 'to': 13, 'europe': 14, 'next': 15, 'week': 16}\n"
     ]
    }
   ],
   "source": [
    "ind_2 = 0\n",
    "for k, v in lem_voc.items():\n",
    "    ind_2 += 1\n",
    "    lem_voc[k] = ind_2\n",
    "    \n",
    "print(lem_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name of Entities vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PERSON': 1, '': 2, 'LOC': 3, 'DATE': 4}\n"
     ]
    }
   ],
   "source": [
    "def gen():\n",
    "    yield nlp(text)\n",
    "\n",
    "matitrial = gen()\n",
    "\n",
    "ent_voc = dict()\n",
    "for tokens in matitrial:\n",
    "    for token in tokens:\n",
    "        ent_voc[token.ent_type_] = 1\n",
    "\n",
    "ind_3 = 0\n",
    "for k, v in ent_voc.items():\n",
    "    ind_3 += 1\n",
    "    ent_voc[k] = ind_3\n",
    "    \n",
    "print(ent_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a concrete example on Vectorize on text data with Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the rotten tamatoes dataset for sentiment analysis\n",
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "5         6           1  of escapades demonstrating the adage that what...   \n",
       "6         7           1                                                 of   \n",
       "7         8           1  escapades demonstrating the adage that what is...   \n",
       "8         9           1                                          escapades   \n",
       "9        10           1  demonstrating the adage that what is good for ...   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  \n",
       "5          2  \n",
       "6          2  \n",
       "7          2  \n",
       "8          2  \n",
       "9          2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/jianwenliu/Documents/IndianaUniversity/courses/nlp/nlphw/train.tsv',sep='\\t')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the limit: if sentiment <3 then 0, otherwise, then sentiment==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianwenliu/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/jianwenliu/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50559</th>\n",
       "      <td>50560</td>\n",
       "      <td>2487</td>\n",
       "      <td>This Chicago</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22563</th>\n",
       "      <td>22564</td>\n",
       "      <td>1020</td>\n",
       "      <td>Orlando Jones</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125529</th>\n",
       "      <td>125530</td>\n",
       "      <td>6745</td>\n",
       "      <td>become a major-league leading lady , -LRB- but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115906</th>\n",
       "      <td>115907</td>\n",
       "      <td>6177</td>\n",
       "      <td>eventually prevail</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2016</td>\n",
       "      <td>77</td>\n",
       "      <td>major problem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124552</th>\n",
       "      <td>124553</td>\n",
       "      <td>6691</td>\n",
       "      <td>insufficiently</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>374</td>\n",
       "      <td>14</td>\n",
       "      <td>say about the ways in which extravagant chance...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17110</th>\n",
       "      <td>17111</td>\n",
       "      <td>743</td>\n",
       "      <td>all the sympathy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143999</th>\n",
       "      <td>144000</td>\n",
       "      <td>7816</td>\n",
       "      <td>to end this flawed , dazzling series with the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56817</th>\n",
       "      <td>56818</td>\n",
       "      <td>2855</td>\n",
       "      <td>qatsi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "50559      50560        2487   \n",
       "22563      22564        1020   \n",
       "125529    125530        6745   \n",
       "115906    115907        6177   \n",
       "2015        2016          77   \n",
       "124552    124553        6691   \n",
       "373          374          14   \n",
       "17110      17111         743   \n",
       "143999    144000        7816   \n",
       "56817      56818        2855   \n",
       "\n",
       "                                                   Phrase  Sentiment  \n",
       "50559                                        This Chicago          0  \n",
       "22563                                       Orlando Jones          0  \n",
       "125529  become a major-league leading lady , -LRB- but...          1  \n",
       "115906                                 eventually prevail          1  \n",
       "2015                                        major problem          0  \n",
       "124552                                     insufficiently          0  \n",
       "373     say about the ways in which extravagant chance...          0  \n",
       "17110                                    all the sympathy          0  \n",
       "143999  to end this flawed , dazzling series with the ...          0  \n",
       "56817                                               qatsi          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=0.05)\n",
    "\n",
    "df.Sentiment[df['Sentiment']<3]=0\n",
    "df.Sentiment[df['Sentiment']>=3]=1\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['PhraseId', 'SentenceId'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clen the text\n",
    "import re\n",
    "def clean(txt):\n",
    "    return re.compile(u'[^a-zA-Z0-9]').sub(' ',txt)   \n",
    "df['Phrase'] = df['Phrase'].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50559                                          This Chicago\n",
       "22563                                         Orlando Jones\n",
       "125529    become a major league leading lady    LRB  but...\n",
       "115906                                   eventually prevail\n",
       "2015                                          major problem\n",
       "124552                                       insufficiently\n",
       "373       say about the ways in which extravagant chance...\n",
       "17110                                      all the sympathy\n",
       "143999    to end this flawed   dazzling series with the ...\n",
       "56817                                                 qatsi\n",
       "Name: Phrase, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#focus on the X ,feature\n",
    "X=df['Phrase']\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the vocab with all\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def gen():\n",
    "    for comm in X:\n",
    "        yield nlp(comm)\n",
    "file = gen()\n",
    "\n",
    "\n",
    "#use dependency parsing\n",
    "def get_dep_voc(file):\n",
    "    dep_voc = dict()\n",
    "    for tokens in file:\n",
    "        for token in tokens:\n",
    "            dep_voc[token.dep_] = 1\n",
    "    return dep_voc\n",
    "\n",
    "def get_count_dep(dep_voc):\n",
    "    ind_5 = 0\n",
    "    for k, v in dep_voc.items():\n",
    "        ind_5 += 1\n",
    "        dep_voc[k] = ind_5\n",
    "    return dep_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_voc=get_dep_voc(file)\n",
    "dep_voc=get_count_dep(dep_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'det': 1, 'ROOT': 2, 'compound': 3, 'punct': 4, 'amod': 5, '': 6, 'intj': 7, 'cc': 8, 'conj': 9, 'advmod': 10, 'prep': 11, 'pobj': 12, 'nsubj': 13, 'aux': 14, 'relcl': 15, 'poss': 16, 'dobj': 17, 'predet': 18, 'acomp': 19, 'ccomp': 20, 'advcl': 21, 'attr': 22, 'mark': 23, 'dep': 24, 'neg': 25, 'auxpass': 26, 'prt': 27, 'dative': 28, 'nummod': 29, 'quantmod': 30, 'xcomp': 31, 'case': 32, 'pcomp': 33, 'nsubjpass': 34, 'nmod': 35, 'agent': 36, 'acl': 37, 'preconj': 38, 'appos': 39, 'oprd': 40, 'npadvmod': 41, 'expl': 42, 'csubj': 43, 'csubjpass': 44, 'meta': 45, 'parataxis': 46}\n"
     ]
    }
   ],
   "source": [
    "print(dep_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data\n",
    "f_train, f_test, l_train, l_test = train_test_split(X, df.Sentiment.values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize on train data\n",
    "def train_gen(dep_voc):\n",
    "    for comm in f_train:\n",
    "        file = nlp(comm)\n",
    "        yield [dep_voc.get(token.dep_,999) for token in file]\n",
    "\n",
    "train_gen = train_gen(dep_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append to the dictionary\n",
    "def get_train_vec(train_gen):\n",
    "    length = 0\n",
    "    train_vec = list()\n",
    "    for vector in train_gen:\n",
    "        train_vec.append(vector)\n",
    "        v_len = len(vector)\n",
    "        if v_len > length:\n",
    "            length = v_len\n",
    "    #padding\n",
    "    i=0\n",
    "    for vector in train_vec:\n",
    "        train_vec[i] = [0] * (length - len(vector)) + vector\n",
    "        i += 1\n",
    "    return train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec=get_train_vec(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# manke numpy array\n",
    "X_train_dep = np.array(train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6242, 52)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the frequence vectorize strategy, we can find ou that NLP features can ba successfully transfered to the vectors. By using Dependency Parsing vectors or POS, we may can also do some sentiment analysis or other NLP task with these vectorized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
